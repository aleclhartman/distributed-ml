{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "import pyspark\n",
    "import pyspark.ml\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(data('tips'))\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(df: pyspark.sql.DataFrame):\n",
    "    return df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We'll first demonstrate a regression problem: predicting the tip amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     12.69| 2.0|  Male|    No|Sat|Dinner|   2|\n",
      "|     13.37| 2.0|  Male|    No|Sat|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyspark.ml.feature.RFormula`\n",
    "\n",
    "- `tip ~ total_bill`: predict tip based on total bill\n",
    "- `tip ~ total_bill + size`: predict tip based on total bill and size\n",
    "- `tip ~ .`: predict tip based on all the other features in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`features` and `labels` columns are the shape/name required for `pyspark.ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find a reference for more details on rformulas\n",
    "# pyspark.ml.feature.RFormula?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+-----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|   features|label|\n",
      "+----------+----+------+------+---+------+----+-----------+-----+\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2| [8.77,2.0]|  2.0|\n",
      "|     12.69| 2.0|  Male|    No|Sat|Dinner|   2|[12.69,2.0]|  2.0|\n",
      "|     13.37| 2.0|  Male|    No|Sat|Dinner|   2|[13.37,2.0]|  2.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|[14.78,2.0]| 3.23|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|[14.83,2.0]| 3.02|\n",
      "+----------+----+------+------+---+------+----+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nb: spark's rformula does encoding\n",
    "rf = pyspark.ml.feature.RFormula(formula=\"tip ~ total_bill + size\").fit(train)\n",
    "rf\n",
    "\n",
    "rf.transform(train).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   features|label|\n",
      "+-----------+-----+\n",
      "| [8.77,2.0]|  2.0|\n",
      "|[12.69,2.0]|  2.0|\n",
      "|[13.37,2.0]|  2.0|\n",
      "|[14.78,2.0]| 3.23|\n",
      "|[14.83,2.0]| 3.02|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input = rf.transform(train).select('features', 'label')\n",
    "train_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create, fit, and use the model.\n",
    "\n",
    "**Note**: unlike `sklearn`, each step produces a new object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_a967cbedd673"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = pyspark.ml.regression.LinearRegression()\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-51959a2378c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "## This Will Not Work!!!!\n",
    "lr = pyspark.ml.regression.LinearRegression()\n",
    "lr.fit(train_input)\n",
    "lr.transform(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------------+\n",
      "|   features|label|        prediction|\n",
      "+-----------+-----+------------------+\n",
      "| [8.77,2.0]|  2.0|1.9717337092055747|\n",
      "|[12.69,2.0]|  2.0|  2.28547899247938|\n",
      "|[13.37,2.0]|  2.0| 2.339904194679938|\n",
      "|[14.78,2.0]| 3.23|2.4527564521840364|\n",
      "|[14.83,2.0]| 3.02| 2.456758305287019|\n",
      "+-----------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_fit = lr.fit(train_input)\n",
    "lr_fit.transform(train_input).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.regression.LinearRegressionTrainingSummary at 0x11d920c18>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.44244602311951, 0.9975913515122248)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.summary.r2, lr_fit.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coefficientStandardErrors',\n",
       " 'degreesOfFreedom',\n",
       " 'devianceResiduals',\n",
       " 'explainedVariance',\n",
       " 'featuresCol',\n",
       " 'labelCol',\n",
       " 'meanAbsoluteError',\n",
       " 'meanSquaredError',\n",
       " 'numInstances',\n",
       " 'objectiveHistory',\n",
       " 'pValues',\n",
       " 'predictionCol',\n",
       " 'predictions',\n",
       " 'r2',\n",
       " 'r2adj',\n",
       " 'residuals',\n",
       " 'rootMeanSquaredError',\n",
       " 'tValues',\n",
       " 'totalIterations']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(lr_fit.summary) if not x.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we do on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+-----+------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|   features|label|        prediction|\n",
      "+----------+----+------+------+---+------+----+-----------+-----+------------------+\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|[10.27,2.0]| 1.71| 2.091789302295041|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|[10.33,3.0]| 1.67|2.3597556280076217|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|[10.34,3.0]| 1.66| 2.360555998628218|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|[15.04,2.0]| 1.96| 2.473566088319544|\n",
      "+----------+----+------+------+---+------+----+-----------+-----+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input = rf.transform(test)\n",
    "lr_fit.transform(test_input).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0581350816287596"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = pyspark.ml.evaluation.RegressionEvaluator()\n",
    "rmse = evaluator.evaluate(lr_fit.transform(test_input))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Predict time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     12.69| 2.0|  Male|    No|Sat|Dinner|   2|\n",
      "|     13.37| 2.0|  Male|    No|Sat|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+-----------+-----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|   features|label|\n",
      "+----------+----+------+------+----+------+----+-----------+-----+\n",
      "|      8.77| 2.0|  Male|    No| Sun|Dinner|   2| [8.77,2.0]|  0.0|\n",
      "|     12.69| 2.0|  Male|    No| Sat|Dinner|   2|[12.69,2.0]|  0.0|\n",
      "|     13.37| 2.0|  Male|    No| Sat|Dinner|   2|[13.37,2.0]|  0.0|\n",
      "|     14.78|3.23|  Male|    No| Sun|Dinner|   2|[14.78,2.0]|  0.0|\n",
      "|     14.83|3.02|Female|    No| Sun|Dinner|   2|[14.83,2.0]|  0.0|\n",
      "|     15.42|1.57|  Male|    No| Sun|Dinner|   2|[15.42,2.0]|  0.0|\n",
      "|     16.29|3.71|  Male|    No| Sun|Dinner|   3|[16.29,3.0]|  0.0|\n",
      "|     16.97| 3.5|Female|    No| Sun|Dinner|   3|[16.97,3.0]|  0.0|\n",
      "|     16.99|1.01|Female|    No| Sun|Dinner|   2|[16.99,2.0]|  0.0|\n",
      "|     17.81|2.34|  Male|    No| Sat|Dinner|   4|[17.81,4.0]|  0.0|\n",
      "|     17.92|4.08|  Male|    No| Sat|Dinner|   2|[17.92,2.0]|  0.0|\n",
      "|     19.65| 3.0|Female|    No| Sat|Dinner|   2|[19.65,2.0]|  0.0|\n",
      "|     19.82|3.18|  Male|    No| Sat|Dinner|   2|[19.82,2.0]|  0.0|\n",
      "|     20.29|2.75|Female|    No| Sat|Dinner|   2|[20.29,2.0]|  0.0|\n",
      "|     20.65|3.35|  Male|    No| Sat|Dinner|   3|[20.65,3.0]|  0.0|\n",
      "|     21.01| 3.5|  Male|    No| Sun|Dinner|   3|[21.01,3.0]|  0.0|\n",
      "|     21.58|3.92|  Male|    No| Sun|Dinner|   2|[21.58,2.0]|  0.0|\n",
      "|      21.7| 4.3|  Male|    No| Sat|Dinner|   2| [21.7,2.0]|  0.0|\n",
      "|     24.59|3.61|Female|    No| Sun|Dinner|   4|[24.59,4.0]|  0.0|\n",
      "|     25.29|4.71|  Male|    No| Sun|Dinner|   4|[25.29,4.0]|  0.0|\n",
      "|     35.26| 5.0|Female|    No| Sun|Dinner|   4|[35.26,4.0]|  0.0|\n",
      "|      9.55|1.45|  Male|    No| Sat|Dinner|   2| [9.55,2.0]|  0.0|\n",
      "|      9.68|1.32|  Male|    No| Sun|Dinner|   2| [9.68,2.0]|  0.0|\n",
      "|      9.94|1.56|  Male|    No| Sun|Dinner|   2| [9.94,2.0]|  0.0|\n",
      "|     10.29| 2.6|Female|    No| Sun|Dinner|   2|[10.29,2.0]|  0.0|\n",
      "|     11.24|1.76|  Male|   Yes| Sat|Dinner|   2|[11.24,2.0]|  0.0|\n",
      "|     12.54| 2.5|  Male|    No| Sun|Dinner|   2|[12.54,2.0]|  0.0|\n",
      "|     15.06| 3.0|Female|    No| Sat|Dinner|   2|[15.06,2.0]|  0.0|\n",
      "|     16.93|3.07|Female|    No| Sat|Dinner|   3|[16.93,3.0]|  0.0|\n",
      "|     17.46|2.54|  Male|    No| Sun|Dinner|   2|[17.46,2.0]|  0.0|\n",
      "|     17.78|3.27|  Male|    No| Sat|Dinner|   2|[17.78,2.0]|  0.0|\n",
      "|     18.04| 3.0|  Male|    No| Sun|Dinner|   2|[18.04,2.0]|  0.0|\n",
      "|     18.29| 3.0|  Male|    No| Sun|Dinner|   2|[18.29,2.0]|  0.0|\n",
      "|     18.35| 2.5|  Male|    No| Sat|Dinner|   4|[18.35,4.0]|  0.0|\n",
      "|     19.49|3.51|  Male|    No| Sun|Dinner|   2|[19.49,2.0]|  0.0|\n",
      "|     20.69|2.45|Female|    No| Sat|Dinner|   4|[20.69,4.0]|  0.0|\n",
      "|     24.06| 3.6|  Male|    No| Sat|Dinner|   3|[24.06,3.0]|  0.0|\n",
      "|     25.56|4.34|  Male|    No| Sun|Dinner|   4|[25.56,4.0]|  0.0|\n",
      "|     28.55|2.05|  Male|    No| Sun|Dinner|   3|[28.55,3.0]|  0.0|\n",
      "|      30.4| 5.6|  Male|    No| Sun|Dinner|   4| [30.4,4.0]|  0.0|\n",
      "|     31.27| 5.0|  Male|    No| Sat|Dinner|   3|[31.27,3.0]|  0.0|\n",
      "|      32.4| 6.0|  Male|    No| Sun|Dinner|   4| [32.4,4.0]|  0.0|\n",
      "|     34.81| 5.2|Female|    No| Sun|Dinner|   4|[34.81,4.0]|  0.0|\n",
      "|     38.01| 3.0|  Male|   Yes| Sat|Dinner|   4|[38.01,4.0]|  0.0|\n",
      "|     48.27|6.73|  Male|    No| Sat|Dinner|   4|[48.27,4.0]|  0.0|\n",
      "|     10.07|1.83|Female|    No|Thur| Lunch|   1|[10.07,1.0]|  1.0|\n",
      "|     10.51|1.25|  Male|    No| Sat|Dinner|   2|[10.51,2.0]|  0.0|\n",
      "|     11.02|1.98|  Male|   Yes| Sat|Dinner|   2|[11.02,2.0]|  0.0|\n",
      "|     13.03| 2.0|  Male|    No|Thur| Lunch|   2|[13.03,2.0]|  1.0|\n",
      "|     13.81| 2.0|  Male|   Yes| Sat|Dinner|   2|[13.81,2.0]|  0.0|\n",
      "+----------+----+------+------+----+------+----+-----------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula='time ~ total_bill + size').fit(train)\n",
    "train_input = rf.transform(train)\n",
    "train_input.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pyspark.ml.classification.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fit = lr.fit(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'areaUnderROC',\n",
       " 'fMeasureByLabel',\n",
       " 'fMeasureByThreshold',\n",
       " 'falsePositiveRateByLabel',\n",
       " 'featuresCol',\n",
       " 'labelCol',\n",
       " 'labels',\n",
       " 'objectiveHistory',\n",
       " 'pr',\n",
       " 'precisionByLabel',\n",
       " 'precisionByThreshold',\n",
       " 'predictionCol',\n",
       " 'predictions',\n",
       " 'probabilityCol',\n",
       " 'recallByLabel',\n",
       " 'recallByThreshold',\n",
       " 'roc',\n",
       " 'totalIterations',\n",
       " 'truePositiveRateByLabel',\n",
       " 'weightedFMeasure',\n",
       " 'weightedFalsePositiveRate',\n",
       " 'weightedPrecision',\n",
       " 'weightedRecall',\n",
       " 'weightedTruePositiveRate']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(lr_fit.summary) if not x.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area Under ROC Curve\n",
    "\n",
    "- Produce a curve where each point on the curve is the TPR vs FPR; multiple points are found by adjusting the threshold for the model [animation](https://stats-demos.zach.wiki/static/roc-auc.mp4)\n",
    "- This works for models that predict a probability in addition to a yes/no\n",
    "- Number between 0 and 1, closer to 1 is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6526599326599327"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# area under TPR (recall) vs FPR (FP / (FP + TN)) curve\n",
    "# https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "lr_fit.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5234521575984991"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = pyspark.ml.evaluation.BinaryClassificationEvaluator()\n",
    "test_auc = evaluator.evaluate(lr_fit.transform(rf.transform(test)))\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+-----------+-----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|   features|label|\n",
      "+----------+----+------+------+----+------+----+-----------+-----+\n",
      "|     10.27|1.71|  Male|    No| Sun|Dinner|   2|[10.27,2.0]|  0.0|\n",
      "|     10.33|1.67|Female|    No| Sun|Dinner|   3|[10.33,3.0]|  0.0|\n",
      "|     10.34|1.66|  Male|    No| Sun|Dinner|   3|[10.34,3.0]|  0.0|\n",
      "|     15.04|1.96|  Male|    No| Sun|Dinner|   2|[15.04,2.0]|  0.0|\n",
      "|     15.77|2.23|Female|    No| Sat|Dinner|   2|[15.77,2.0]|  0.0|\n",
      "|     18.43| 3.0|  Male|    No| Sun|Dinner|   4|[18.43,4.0]|  0.0|\n",
      "|     23.68|3.31|  Male|    No| Sun|Dinner|   2|[23.68,2.0]|  0.0|\n",
      "|     26.88|3.12|  Male|    No| Sun|Dinner|   4|[26.88,4.0]|  0.0|\n",
      "|     39.42|7.58|  Male|    No| Sat|Dinner|   4|[39.42,4.0]|  0.0|\n",
      "|     13.94|3.06|  Male|    No| Sun|Dinner|   2|[13.94,2.0]|  0.0|\n",
      "|     16.04|2.24|  Male|    No| Sat|Dinner|   3|[16.04,3.0]|  0.0|\n",
      "|     16.31| 2.0|  Male|    No| Sat|Dinner|   3|[16.31,3.0]|  0.0|\n",
      "|     18.69|2.31|  Male|    No| Sat|Dinner|   3|[18.69,3.0]|  0.0|\n",
      "|     22.23| 5.0|  Male|    No| Sun|Dinner|   2|[22.23,2.0]|  0.0|\n",
      "|     26.41| 1.5|Female|    No| Sat|Dinner|   2|[26.41,2.0]|  0.0|\n",
      "|      3.07| 1.0|Female|   Yes| Sat|Dinner|   1| [3.07,1.0]|  0.0|\n",
      "|     12.02|1.97|  Male|    No| Sat|Dinner|   2|[12.02,2.0]|  0.0|\n",
      "|     15.01|2.09|  Male|   Yes| Sat|Dinner|   2|[15.01,2.0]|  0.0|\n",
      "|     16.66| 3.4|  Male|    No|Thur| Lunch|   2|[16.66,2.0]|  1.0|\n",
      "|     22.76| 3.0|  Male|    No|Thur| Lunch|   2|[22.76,2.0]|  1.0|\n",
      "+----------+----+------+------+----+------+----+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input = rf.transform(test)\n",
    "test_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "|prediction|0.0| 1.0|\n",
      "+----------+---+----+\n",
      "|       0.0| 40|  13|\n",
      "|       1.0|  1|null|\n",
      "+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for the test data\n",
    "(lr_fit.transform(test_input)\n",
    " .select('time', 'total_bill', 'size', 'label', 'probability', 'prediction')\n",
    " .groupby('prediction') # predicted == rows\n",
    " .pivot('label') # actual values are columns\n",
    " .count()\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many other preprocessing steps\n",
    "# dir(pyspark.ml.feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we create a baseline?\n",
    "\n",
    "For our regression model: baseline prediction is the average tip amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tip_amount = train.agg(mean('tip')).head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|       prediction|\n",
      "+----------+----+------+------+---+------+----+-----------------+\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|3.005105263157895|\n",
      "|     12.69| 2.0|  Male|    No|Sat|Dinner|   2|3.005105263157895|\n",
      "|     13.37| 2.0|  Male|    No|Sat|Dinner|   2|3.005105263157895|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|3.005105263157895|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|3.005105263157895|\n",
      "+----------+----+------+------+---+------+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(train.selectExpr('*', '{} as prediction'.format(avg_tip_amount))\n",
    " .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.33600848542597"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula=\"tip ~ total_bill + size\").fit(train)\n",
    "\n",
    "baseline_eval_df = (\n",
    "    rf.transform(train)\n",
    "    .selectExpr('label', '{} as prediction'.format(avg_tip_amount))\n",
    "    .select('label', col('prediction').cast('float'))\n",
    ")\n",
    "\n",
    "evaluator = pyspark.ml.evaluation.RegressionEvaluator()\n",
    "baseline_rmse = evaluator.evaluate(baseline_eval_df)\n",
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula='time ~ total_bill + size').fit(train)\n",
    "train_input = rf.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|  135|\n",
      "|  1.0|   55|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+----------------------------------------+\n",
      "|prediction|rawPrediction                             |probability                             |\n",
      "+----------+------------------------------------------+----------------------------------------+\n",
      "|0.0       |[0.24391504551760637,-0.24391504551760637]|[0.5606782239589314,0.43932177604106865]|\n",
      "|0.0       |[0.5224097706862575,-0.5224097706862575]  |[0.6277110784259653,0.37228892157403476]|\n",
      "|0.0       |[0.5707200801542888,-0.5707200801542888]  |[0.6389293132787042,0.3610706867212957] |\n",
      "|0.0       |[0.6708929277277066,-0.6708929277277066]  |[0.66170307181667,0.33829692818333]     |\n",
      "|0.0       |[0.6744451563650621,-0.6744451563650621]  |[0.6624977883898066,0.33750221161019345]|\n",
      "|0.0       |[0.716361454285854,-0.716361454285854]    |[0.6718052818590612,0.3281947181409388] |\n",
      "|0.0       |[0.6684740631485411,-0.6684740631485411]  |[0.6611613922536789,0.33883860774632113]|\n",
      "|0.0       |[0.7167843726165726,-0.7167843726165726]  |[0.6718985213620305,0.3281014786379695] |\n",
      "|0.0       |[0.8279014334988085,-0.8279014334988085]  |[0.6959110162145502,0.30408898378544974]|\n",
      "|0.0       |[0.6667656442968466,-0.6667656442968466]  |[0.6607785549800277,0.33922144501997237]|\n",
      "|0.0       |[0.8939728861536161,-0.8939728861536161]  |[0.709709355888092,0.29029064411190797] |\n",
      "|0.0       |[1.0168799970061073,-1.0168799970061073]  |[0.7343644153298614,0.26563558467013854]|\n",
      "|0.0       |[1.0289575743731152,-1.0289575743731152]  |[0.7367137499773295,0.26328625002267053]|\n",
      "|0.0       |[1.0623485235642545,-1.0623485235642545]  |[0.7431390953433143,0.25686090465668565]|\n",
      "|0.0       |[0.9782284003259184,-0.9782284003259184]  |[0.7267565508432193,0.27324344915678067]|\n",
      "|0.0       |[1.0038044465148763,-1.0038044465148763]  |[0.7318059203620491,0.268194079637951]  |\n",
      "|0.0       |[1.1539960224080197,-1.1539960224080197]  |[0.7602400496724093,0.23975995032759062]|\n",
      "|0.0       |[1.1625213711376725,-1.1625213711376725]  |[0.7617905591965222,0.23820944080347786]|\n",
      "|0.0       |[1.1484478475222177,-1.1484478475222177]  |[0.7592272957742644,0.2407727042257356] |\n",
      "|0.0       |[1.198179048445191,-1.198179048445191]    |[0.7682006879570861,0.23179931204291404]|\n",
      "+----------+------------------------------------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_fit.transform(train_input).select('prediction', 'rawPrediction', 'probability').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'BinaryClassificationEvaluator_b62d26410254 parameter metricName given invalid value accuracy.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1488.w.\n: java.lang.IllegalArgumentException: BinaryClassificationEvaluator_b62d26410254 parameter metricName given invalid value accuracy.\n\tat org.apache.spark.ml.param.Param.validate(params.scala:77)\n\tat org.apache.spark.ml.param.ParamPair.<init>(params.scala:656)\n\tat org.apache.spark.ml.param.Param.$minus$greater(params.scala:87)\n\tat org.apache.spark.ml.param.Param.w(params.scala:83)\n\tat sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-aad749f240b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbaseline_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_eval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbaseline_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mjava_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'BinaryClassificationEvaluator_b62d26410254 parameter metricName given invalid value accuracy.'"
     ]
    }
   ],
   "source": [
    "baseline_eval_df =  (\n",
    "    train_input.selectExpr('label', '0 as rawPrediction')\n",
    ")\n",
    "\n",
    "evaluator = pyspark.ml.evaluation.BinaryClassificationEvaluator(metricName='accuracy')\n",
    "baseline_auc = evaluator.evaluate(baseline_eval_df)\n",
    "baseline_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
